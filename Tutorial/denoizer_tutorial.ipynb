{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd4a29e-387b-40f5-92b6-4ff4845e9941",
   "metadata": {},
   "source": [
    "![ThinkOnward Logo](https://github.com/thinkonward/geophysical-foundation-model/blob/2848c9ae410b6bc334138f5689cb2a3b15fd02a6/Tutorial/assets/ThinkOnward.png?raw=True)\n",
    "\n",
    "# Seismic Denoising: Fine Tune the Geophysical Foundation Model\n",
    "\n",
    "### Jesse Pisel March 2025\n",
    "\n",
    "Make sure you install the required packages from requirements.txt as outlined in the README file in the repository root directory. Note that this tutorial does not include complete code for training your own fine-tuned model for denoising. The ThinkOnward `denoizer` model is available on Hugging Face for you to use in this tutorial. It was the baseline model for the [Image Impeccable Challenge](https://thinkonward.com/app/c/challenges/image-impeccable/leaderboard) with a score of `0.901743`. We strongly recommend training your own model to see what you can improve.\n",
    "\n",
    "## First Steps\n",
    "First you need to import packages needed for data wrangling and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f240bcd-18e4-4327-aa2d-a1301f379a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81659e4-eecf-491f-a814-94712ec4083e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "from GFM import ElasticViTMAE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd5239d-79fc-4867-8c4e-70cb8d622dbe",
   "metadata": {},
   "source": [
    "## Hugging Face Access\n",
    "\n",
    "To download the `denoizer` model you will need a [HuggingFace](https://huggingface.co/) account and [request model access](https://huggingface.co/thinkonward/denoizer). \n",
    "\n",
    "After requesting access, you need to [set up an access token](https://huggingface.co/docs/hub/security-tokens) to use the `huggingface_hub` package. Once that is set up, log in by running the next cell and entering your token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7366c4e6-c708-4095-8c91-ca9e633b5783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login, hf_hub_download, snapshot_download\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93288ef-14fb-4145-890e-a93c88afdef7",
   "metadata": {},
   "source": [
    "## Fine Tuning Considerations\n",
    "\n",
    "As mentioned above, you should train your own fine-tuned model from the GFM. To do so you should follow the following guidelines:\n",
    "\n",
    "1. You will need to build a paired dataset with noise and denoised examples. \n",
    "\n",
    "Check out the [Image Impeccable: Journey to Clarity Challenge](https://thinkonward.com/app/c/challenges/image-impeccable) for some ideas or [just use the data](https://forum.thinkonward.com/t/image-impeccable-training-data-links-to-open-s3-bucket/2171)\n",
    "\n",
    "2. You will need to build a way to evaluate your model and track its loss\n",
    "3. You will need to pick which layers you want to freeze, and which ones you want to train for fine tuning e.g.\n",
    "\n",
    "```python\n",
    "# first set all layers to non trainable\n",
    "    for n, p in model.named_parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    if args.trainable_layers == 'head':\n",
    "        for n, p in model.named_parameters():\n",
    "            if n.startswith('decoder_pred'):\n",
    "                p.requires_grad = True\n",
    "    else:\n",
    "        for n, p in model.named_parameters():\n",
    "            if n.startswith('decoder_pos_embed'): # pos embed should not be trainable\n",
    "                pass\n",
    "            elif n.startswith('decoder') or n.startswith('mask_token'):\n",
    "                p.requires_grad = True\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        print(n, p.shape, 'trainable:', p.requires_grad)\n",
    "```\n",
    "4. For your `CustomDataset` dataloader we strongly suggest checking out the [Image Impeccable Starter Notebook](https://github.com/thinkonward/challenges/tree/main/geoscience/image-impeccable/image-impeccable-starter-notebook) for some ideas\n",
    "5. Start your training, and check to see that the model is improving over time\n",
    "\n",
    "For this tutorial we will use a fine-tuned version of the GFM that the ThinkOnward Challenges team trained on the Image Impeccable Challenge dataset. Lots of code will be similar to the interpolation task, but there will be some notable differences. The model used in this tutorial is an example of fine tuning and **should not be used without additional training** for your use case.\n",
    "\n",
    "## Model Architecture Changes\n",
    "Let's talk model changes, \n",
    "you need to edit three methods in `ElasticViTMAE.py` for the `ElasticViTMAE` class.\n",
    "\n",
    "**1. `forward_encoder`**\n",
    "    \n",
    "* Original GFM forward encoder has random masking for training:\n",
    "\n",
    "```python\n",
    "def forward_encoder(self, x, idx_shuffle, len_keep):\n",
    "    # embed patches\n",
    "    x = self.patch_embed(x)\n",
    "\n",
    "    # add pos embed w/o cls token\n",
    "    x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "    # masking: length -> length * mask_ratio\n",
    "    x, mask, ids_restore = self.random_masking(x, idx_shuffle, len_keep)\n",
    "\n",
    "    # append cls token\n",
    "    cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "    cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "    x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "    # apply Transformer blocks\n",
    "    for blk in self.blocks:\n",
    "        x = blk(x)\n",
    "    x = self.norm(x)\n",
    "\n",
    "    return x, mask, ids_restore\n",
    "```\n",
    "\n",
    "* New denoiser forward encoder has the random masking removed so no longer needs idx_shuffle or len_keep:\n",
    "\n",
    "```python\n",
    "def forward_encoder(self, x):\n",
    "    # embed patches\n",
    "    x = self.patch_embed(x)\n",
    "\n",
    "    # add pos embed w/o cls token\n",
    "    x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "    # append cls token\n",
    "    cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "    cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "    x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "    # apply Transformer blocks\n",
    "    for blk in self.blocks:\n",
    "        x = blk(x)\n",
    "    x = self.norm(x)\n",
    "\n",
    "    return x\n",
    "```\n",
    "\n",
    "**2. `forward_decoder`**\n",
    "\n",
    "* Original GFM forward decoder had shuffling and image restoration for masking:\n",
    "\n",
    "```python\n",
    "def forward_decoder(self, x, ids_restore):\n",
    "    # embed tokens\n",
    "    x = self.decoder_embed(x)\n",
    "\n",
    "    # append mask tokens to sequence\n",
    "    mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "    x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "    x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "    x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "    # add pos embed\n",
    "    x = x + self.decoder_pos_embed\n",
    "\n",
    "    # apply Transformer blocks\n",
    "    for blk in self.decoder_blocks:\n",
    "        x = blk(x)\n",
    "    x = self.decoder_norm(x)\n",
    "\n",
    "    # predictor projection\n",
    "    x = self.decoder_pred(x)\n",
    "\n",
    "    if not self.custom_head:\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "    return x\n",
    "```\n",
    "\n",
    "* New denoiser has the unshuffle removed and no longer takes ids_restore so we set that to image width of 160:\n",
    "\n",
    "```python\n",
    "def forward_decoder(self, x):\n",
    "    # embed tokens\n",
    "    x = self.decoder_embed(x)\n",
    "\n",
    "    # append mask tokens to sequence\n",
    "    mask_tokens = self.mask_token.repeat(x.shape[0], 160 + 1 - x.shape[1], 1)\n",
    "    x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "    # remove the unshuffle \n",
    "    x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "    # add pos embed\n",
    "    x = x + self.decoder_pos_embed\n",
    "\n",
    "    # apply Transformer blocks\n",
    "    for blk in self.decoder_blocks:\n",
    "        x = blk(x)\n",
    "    x = self.decoder_norm(x)\n",
    "\n",
    "    # predictor projection\n",
    "    x = self.decoder_pred(x)\n",
    "\n",
    "    if not self.custom_head:\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "    return x\n",
    "```\n",
    "\n",
    "**3. `forward` method**\n",
    "\n",
    "* Original GFM masked traces and tracked loss:\n",
    "\n",
    "```python\n",
    "def forward(self, imgs, idx_shuffle, len_keep):\n",
    "    latent, mask, ids_restore = self.forward_encoder(imgs, idx_shuffle, len_keep)\n",
    "    pred = self.forward_decoder(latent, ids_restore)\n",
    "    loss = self.forward_loss(imgs, pred, mask)\n",
    "    return loss, pred, mask\n",
    "```\n",
    "\n",
    "* Denoizer no longer needs to mask traces or track the loss for masking so remove those. You will need to build a new way to track loss for your classification such as `CrossEntropyLoss` when you fine tune the model:\n",
    "\n",
    "```python\n",
    "def forward(self, img):\n",
    "    latent = self.forward_encoder(img)\n",
    "    pred = self.forward_decoder(latent)\n",
    "    return pred\n",
    "```\n",
    "\n",
    "After saving those changes in the GFM architecture code you are all set to load up the fine-tuned model and run inference for denoising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e654e26-58b9-4831-a27e-9b5594fd6311",
   "metadata": {},
   "source": [
    "## Loading the fine-tuned model\n",
    "\n",
    "For an example of how to use a fine tuned version of the GFM you will need to download the model weights and create an instance of the `ElasticViTMAE` class. Once it is instantiated, you will then send it to the CUDA device and set it to eval mode so you can run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183b3f0f-b0ee-4d4d-bf68-5ce0532f88c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from GFM import ElasticViTMAE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "denoiser = ElasticViTMAE.ElasticViTMAE.from_pretrained(\"thinkonward/denoizer\")\n",
    "denoiser = denoiser.float()\n",
    "denoiser.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5ee00-56cb-4a24-ab88-516c63fcf316",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "\n",
    "Next you need some data to work with. The cell below downloads some of the [Image Impeccable Challenge](https://thinkonward.com/app/c/challenges/image-impeccable) seismic data for you to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a90c7-a24f-4fe3-ad29-35068931691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_REPO_ID = \"thinkonward/image-impeccable\"\n",
    "\n",
    "NOISY_DATA = \"train/42487393/seismic_w_noise_vol_42487393.parquet\"\n",
    "CLEAN_DATA = \"train/42487393/seismicCubes_RFC_fullstack_2024.42487393.parquet\"\n",
    "\n",
    "hf_hub_download(\n",
    "    repo_id=DATA_REPO_ID,\n",
    "    filename=NOISY_DATA,\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=\"./dataset\",\n",
    ")\n",
    "\n",
    "hf_hub_download(\n",
    "    repo_id=DATA_REPO_ID,\n",
    "    filename=CLEAN_DATA,\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=\"./dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df62363-f7f5-4b87-873f-f68adfdaff6a",
   "metadata": {},
   "source": [
    "## Load a test dataset\n",
    "\n",
    "Now that the model is downloaded and loaded to your CUDA device, you will need to load up a single seismic volume. It helps to visualize what the noise looks like in an inline, crossline, and timeslice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67afed63-53cd-4286-abea-86a60e5953da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parquet2array(parquet_file, original_shape=(1259,300,300)):\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    data_only = df.drop(columns=['Row', 'Col'])\n",
    "    # Convert the DataFrame back to a 2D numpy array\n",
    "    reshaped_array = data_only.values\n",
    "    # Reshape the 2D array back into a 3D array\n",
    "    array = reshaped_array.reshape(original_shape)\n",
    "    return array\n",
    "\n",
    "def visualize_seismic(volume, inline, crossline, timeslice):\n",
    "    \"\"\"\n",
    "    Visualize a seismic volume by displaying three orthogonal slices.\n",
    "\n",
    "    Parameters:\n",
    "    volume (3D array): The seismic volume data.\n",
    "    inline (int): The index of the inline slice to display.\n",
    "    crossline (int): The index of the crossline slice to display.\n",
    "    timeslice (int): The index of the timeslice to display.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Notes:\n",
    "    The function displays three subplots:\n",
    "    - The first subplot shows the inline slice with a red dashed line indicating the timeslice.\n",
    "    - The second subplot shows the crossline slice with a red dashed line indicating the timeslice.\n",
    "    - The third subplot shows the timeslice with red dashed lines indicating the inline and crossline indices.\n",
    "    \"\"\"\n",
    "\n",
    "    iline = volume.T[:, inline, :]\n",
    "    xline = volume.T[crossline, :, :]\n",
    "    ts = volume.T[:, :, timeslice]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(10, 6), sharex=True)\n",
    "    ax[0].imshow(iline.T, cmap=\"gray\") #view from left of timeslice\n",
    "    ax[0].axhline(y=timeslice, color='r', linestyle='--')\n",
    "    ax[0].set_title(f\"Inline {inline}\")\n",
    "\n",
    "    ax[1].imshow(xline.T, cmap=\"gray\") #view from bottom of timeslice\n",
    "    ax[1].axhline(y=timeslice, color='r', linestyle='--')\n",
    "    ax[1].set_title(f\"Cross-line {crossline}\")\n",
    "\n",
    "    ax[2].imshow(ts, cmap=\"gray\")\n",
    "    ax[2].axhline(y=crossline, color='r', linestyle='--')\n",
    "    ax[2].axvline(x=inline, color='r', linestyle='--')\n",
    "    ax[2].set_title(f\"Timeslice {timeslice}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "SEISMIC_VOL = parquet2array(glob(f'./dataset/train/42487393/seismic_w_*.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb573ea3-33b9-491c-a3ca-9a1fd5167b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_seismic(SEISMIC_VOL, 80, 200, 650)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a468969-d81a-4ae1-8b74-9c9c67ef5ae9",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Now you will have the model work through the seismic volume slice by slice and make predictions on each slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e871ac-4e25-466c-8b10-6550e3d48cea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start by rescaling the entire volume\n",
    "minval = np.nanmin(SEISMIC_VOL)\n",
    "maxval = np.nanmax(SEISMIC_VOL)\n",
    "seismic = ((SEISMIC_VOL - minval) / (maxval - minval)) * 255\n",
    "\n",
    "# next work through each slice and denoise it\n",
    "# you might try orthogonal directions to see how it changes results\n",
    "for i in range(seismic.shape[-1]):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        # resize\n",
    "        input_sample = cv2.resize(seismic[:, :, i], (160, 400))\n",
    "        # standardize\n",
    "        input_sample = (input_sample - 128.) / 43.\n",
    "\n",
    "        x = torch.tensor(input_sample).unsqueeze(0).unsqueeze(0)\n",
    "        x = x.float()\n",
    "        x = x.to(device)\n",
    "\n",
    "        pred = denoiser(x)\n",
    "        # scale back 0-255\n",
    "        pred = pred * 43. + 128.\n",
    "\n",
    "        pred = pred.cpu().detach().numpy()[0].T\n",
    "        pred = cv2.resize(pred.astype('float32'), (300, 1259))\n",
    "\n",
    "        seismic[:, :, i] = pred\n",
    "\n",
    "SEISMIC_PRED = np.clip(seismic, 0, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394c356b-8166-4fd4-9d5e-df410f598012",
   "metadata": {},
   "source": [
    "## Visualise results for QC\n",
    "\n",
    "Now you will look at the denoised seismic to see how the model did. There is still quite a bit of noise, lets see what the ground truth data looks like for a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90afa97-a374-470a-a387-9e17e1f55089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_seismic(SEISMIC_PRED, 80, 200, 650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc031892-88d8-48a8-9a70-c56cea7457ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ORIGINAL = parquet2array(glob(f'./dataset/train/42487393/seismicCubes*.parquet'))\n",
    "visualize_seismic(ORIGINAL, 80, 200, 650)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268feaf0-5789-4bf0-919d-8102ad1f9d31",
   "metadata": {},
   "source": [
    "Some thoughts: you might want to try predicting the noise volume instead of the denoised volume, or [check out the Image Impeccable winning solutions on GitHub](https://github.com/thinkonward/challenges/tree/main/geoscience/image-impeccable) for more ideas on cutting edge denoising solutions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
